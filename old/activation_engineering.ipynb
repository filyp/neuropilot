{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do mean, but don't collapse across the 2nd axis\n",
    "# act_diff = act_diff.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original activation engineering code: https://colab.research.google.com/drive/1y84fhgkGX0ft2DmYJB3K13lAyf-0YonK?usp=sharing#scrollTo=ZExJFurIjKHM\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Dict, Union, List\n",
    "\n",
    "# load the model\n",
    "torch.set_grad_enabled(False)  # save memory\n",
    "# # https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h\n",
    "# model = HookedTransformer.from_pretrained(\"distilgpt2\", device=\"cpu\")   # 88M, loss=4.1, 9it/s\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")   # 85M, loss=3.7, 7it/s\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=\"cpu\")   # 300M, loss=3.4, 3.6it/s\n",
    "# model = HookedTransformer.from_pretrained(\"pythia-410m-deduped\", device=\"cpu\", checkpoint_index=153) # 410M, loss=3.1, 2.5it/s\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-large\", device=\"cpu\")   # 700M, loss=3.3, 1.1it/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "\n",
    "# Specific to the love/hate example\n",
    "prompt_add, prompt_sub = \"Love\", \"Hate\"\n",
    "coeff = 5\n",
    "act_name = 6\n",
    "prompt = \"I hate you because\"\n",
    "\n",
    "# padding\n",
    "tlen = lambda prompt: model.to_tokens(prompt).shape[1]\n",
    "pad_right = lambda prompt, length: prompt + \" \" * (length - tlen(prompt))\n",
    "l = max(tlen(prompt_add), tlen(prompt_sub))\n",
    "prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)\n",
    "print(f\"'{prompt_add}'\", f\"'{prompt_sub}'\")\n",
    "\n",
    "# get activations\n",
    "def get_resid_pre(prompt: str, layer: int):\n",
    "    name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    cache, caching_hooks, _ = model.get_caching_hooks(lambda n: n == name)\n",
    "    with model.hooks(fwd_hooks=caching_hooks):\n",
    "        _ = model(prompt)\n",
    "    return cache[name]\n",
    "\n",
    "\n",
    "act_add = get_resid_pre(prompt_add, act_name)\n",
    "act_sub = get_resid_pre(prompt_sub, act_name)\n",
    "act_diff = act_add - act_sub\n",
    "print(act_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the modified model\n",
    "\n",
    "def ave_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "\n",
    "    # We only add to the prompt (first call), not the generated tokens.\n",
    "    ppos, apos = resid_pre.shape[1], act_diff.shape[1]\n",
    "    assert apos <= ppos, f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "\n",
    "    # add to the beginning (position-wise) of the activations\n",
    "    resid_pre[:, :apos, :] += coeff * act_diff\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch: List[str], fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        r = model.generate(input=tokenized, max_new_tokens=20, do_sample=True, **kwargs)\n",
    "    return r\n",
    "\n",
    "\n",
    "editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", ave_hook)]\n",
    "res = hooked_generate([prompt] * 4, editing_hooks, seed=SEED, **sampling_kwargs)\n",
    "\n",
    "# Print results, removing the ugly beginning of sequence token\n",
    "res_str = model.to_string(res[:, 1:])\n",
    "print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
