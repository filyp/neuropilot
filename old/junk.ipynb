{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is too slow\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga-7B\", use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga-7B\", torch_dtype=torch.float32, low_cpu_mem_usage=True, device_map=\"auto\", offload_folder=\"offload_folder\")\n",
    "system_prompt = \"### System:\\nYou are StableBeluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n",
    "\n",
    "message = \"Write me a poem please\"\n",
    "prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from\n",
    "# but I want to use CPU\n",
    "# instead call the model like this:\n",
    "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga-7B\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "# <style>\n",
    "# .cell-output-ipywidget-background {\n",
    "#    background-color: transparent !important;\n",
    "# }\n",
    "# .jp-OutputArea-output {\n",
    "#    background-color: transparent;\n",
    "# }  \n",
    "# </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# text = widgets.Textarea(\n",
    "#     value='Hello World',\n",
    "#     description='String:',\n",
    "#     layout=widgets.Layout(width=\"50%\", height=\"300px\"),\n",
    "# )\n",
    "# display(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_hook(resid_pre, hook):\n",
    "    coeff = 5\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "    # We only add to the prompt (first call), not the generated tokens.\n",
    "    ppos, apos = resid_pre.shape[1], act_diff.shape[1]\n",
    "    assert apos <= ppos, f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "    # add to the beginning (position-wise) of the activations\n",
    "    resid_pre[:, :apos, :] += coeff * act_diff\n",
    "\n",
    "act_name = 6\n",
    "editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", ave_hook)]\n",
    "with model.hooks(fwd_hooks=editing_hooks):\n",
    "    # tokenized = model.to_tokens(prompt_batch)\n",
    "    # r = model.generate(input=tokenized, max_new_tokens=20, do_sample=True, **kwargs)\n",
    "    new_text = model.generate(text.value, max_new_tokens=1, temperature=1, verbose=False)\n",
    "    text.value = new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dims(resid_pre, hook):\n",
    "    print(resid_pre.shape, hook)\n",
    "    # resid_pre[:, -1, :] *= 0.5\n",
    "    # get norm of this vector\n",
    "    norm = torch.norm(resid_pre[:, -1, :], dim=-1, keepdim=True)\n",
    "    print(norm)\n",
    "    print(resid_pre)\n",
    "\n",
    "\n",
    "act_name = 6\n",
    "editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", get_dims)]\n",
    "with model.hooks(fwd_hooks=editing_hooks):\n",
    "    # tokenized = model.to_tokens(prompt_batch)\n",
    "    # r = model.generate(input=tokenized, max_new_tokens=20, do_sample=True, **kwargs)\n",
    "    new_text = model.generate(\"this is some\", max_new_tokens=10, temperature=1, verbose=False)\n",
    "    # text.value = new_text\n",
    "    print(new_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
