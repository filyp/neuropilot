{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original activation engineering code: https://colab.research.google.com/drive/1y84fhgkGX0ft2DmYJB3K13lAyf-0YonK?usp=sharing#scrollTo=ZExJFurIjKHM\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Dict, Union, List, Tuple\n",
    "\n",
    "# load the model\n",
    "torch.set_grad_enabled(False)  # save memory\n",
    "# # https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h\n",
    "# model = HookedTransformer.from_pretrained(\"distilgpt2\", device=\"cpu\")   # 88M, loss=4.1, 9it/s\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")   # 85M, loss=3.7, 7it/s\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=\"cpu\")   # 300M, loss=3.4, 3.6it/s\n",
    "# model = HookedTransformer.from_pretrained(\"pythia-410m-deduped\", device=\"cpu\", checkpoint_index=153) # 410M, loss=3.1, 2.5it/s\n",
    "#\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-large\", device=\"cpu\")   # 700M, loss=3.3, 1.1it/s\n",
    "\n",
    "num_layers = len(model._modules[\"blocks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput import keyboard\n",
    "# \"keyboard\" lib would let us access what is pressed directly, but it requires root\n",
    "\n",
    "\n",
    "class KeyHandler:\n",
    "    def __init__(self, toggling_key=\"cmd\"):\n",
    "        # toggling_key is the key that will be used to toggle the generating lock\n",
    "        # good candidates are \"cmd\", \"alt\", \"ctrl\"\n",
    "        self._toggling_key = toggling_key\n",
    "        self.pressed = set()\n",
    "        self.esc_registered = False\n",
    "        self._toggle_key_pressed_alone = False\n",
    "        self._generating_lock = False\n",
    "        self.listener = keyboard.Listener(on_press=self._on_press, on_release=self._on_release)\n",
    "        self.listener.start()\n",
    "\n",
    "    def _on_press(self, key):\n",
    "        k = str(key).replace(\"'\", \"\").replace(\"Key.\", \"\").replace(\"<65511>\", \"alt\").lower()\n",
    "        if k == \"<0>\":\n",
    "            return    # this is some weird macro artifact\n",
    "        if k == \"esc\":\n",
    "            self.esc_registered = True\n",
    "        # print(f\"key {k} pressed\") \n",
    "        self.pressed.add(k)\n",
    "        \n",
    "        # implement toggling behavior\n",
    "        if k == self._toggling_key and len(self.pressed) == 1:\n",
    "            self._toggle_key_pressed_alone = True\n",
    "        if k != self._toggling_key:\n",
    "            self._toggle_key_pressed_alone = False\n",
    "\n",
    "    def _on_release(self, key): \n",
    "        k = str(key).replace(\"'\", \"\").replace(\"Key.\", \"\").replace(\"<65511>\", \"alt\").lower()\n",
    "        # print(f\"key {k} released\") \n",
    "        if k in self.pressed:\n",
    "            self.pressed.remove(k)\n",
    "\n",
    "        # implement toggling behavior\n",
    "        if k == self._toggling_key and self._toggle_key_pressed_alone:\n",
    "            # toggle key was tapped w/o anything else\n",
    "            self._generating_lock = not self._generating_lock\n",
    "            # it's unclean to reference ui here, but it's the easiest way\n",
    "            # I coulc also use a callback\n",
    "            ui.text_area.disabled = self._generating_lock\n",
    "\n",
    "    def should_we_stop_generating(self, *args):\n",
    "        # returning True means stop\n",
    "        if self.esc_registered:\n",
    "            # just to be sure esc can always stop; maybe not needed\n",
    "            return True\n",
    "\n",
    "        if self._generating_lock:\n",
    "            # if generating lock is on, we don't want to stop\n",
    "            return False\n",
    "        if \"alt\" in self.pressed and \"shift\" in self.pressed:\n",
    "            return False\n",
    "        # no reason to continue generating\n",
    "        return True\n",
    "                \n",
    "\n",
    "key_handler = KeyHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import panel as pn\n",
    "\n",
    "\n",
    "class UI:\n",
    "    def __init__(self):\n",
    "        self.text_area = pn.widgets.TextAreaInput(value=\"\", sizing_mode=\"stretch_both\")\n",
    "        self.steering_strength = pn.widgets.FloatSlider(name=\"Steering Strength\", start=0.0, end=30, step=0.01, value=3)\n",
    "        self.layer_num = pn.widgets.IntSlider(name=\"Layer Number\", start=0, end=num_layers - 1, step=1, value=6)\n",
    "        self.info_box = pn.widgets.StaticText(name=\"Info\", value=\"Not started\")\n",
    "\n",
    "        # some square for 2d plotting, must have square aspect ratio\n",
    "        # turn interactive plotting off, so that it's not displayed in notebook\n",
    "        plt.ioff()\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "        ax.set_aspect(\"equal\") # note: not sure if clear resets this or not\n",
    "        ax.set_facecolor((0., 0., 0.))  # black background\n",
    "        self._ax = ax\n",
    "        self._max_plotting_scale = 0.000001\n",
    "        self.update_plot(None, None)  # set up plot\n",
    "        self.plot = pn.pane.Matplotlib(fig, tight=True, sizing_mode=\"stretch_both\", format=\"svg\")\n",
    "        # svg format is necessary; without it there are some weird lags when updating the text!\n",
    "        \n",
    "        self.full = pn.Row(\n",
    "            self.text_area,\n",
    "            pn.Column(\n",
    "                pn.Row(self.steering_strength, self.layer_num, sizing_mode=\"stretch_width\"),\n",
    "                self.info_box,\n",
    "                self.plot,\n",
    "                sizing_mode=\"stretch_both\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    def update_plot(self, existing_activation: List[float], modifying_activations: List[Tuple[List[float], str]]):\n",
    "        ax = self._ax\n",
    "        # clear previous plot\n",
    "        ax.clear()\n",
    "        # plot formatting\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # ax.set_title(\"Click to start\")\n",
    "        # draw axis lines\n",
    "        ax.plot([-1, 1], [0, 0], color=\"grey\", linewidth=1)\n",
    "        ax.plot([0, 0], [-1, 1], color=\"grey\", linewidth=1)\n",
    "        if existing_activation is None:\n",
    "            return\n",
    "        \n",
    "        # update scale\n",
    "        vector_sum = np.array(existing_activation[:2])\n",
    "        self._max_plotting_scale = max(np.abs(vector_sum[0]), np.abs(vector_sum[1]), self._max_plotting_scale)\n",
    "        for activation, _ in modifying_activations:\n",
    "            vector_sum += np.array(activation[:2])\n",
    "            self._max_plotting_scale = max(np.abs(vector_sum[0]), np.abs(vector_sum[1]), self._max_plotting_scale)\n",
    "        s = self._max_plotting_scale\n",
    "\n",
    "        # draw existing activation\n",
    "        vector_sum = np.array(existing_activation[:2])\n",
    "        # ax.arrow(0, 0, vector_sum[0] / s, vector_sum[1] / s, color=\"white\", linewidth=4, head_width=0.04, head_length=0.04)\n",
    "        ax.plot([0, vector_sum[0] / s], [0, vector_sum[1] / s], color=\"white\", linewidth=4)\n",
    "        # draw modifying activations\n",
    "        for activation, key in modifying_activations:\n",
    "            # convert key (string) to color by hashing\n",
    "            hue = int(hashlib.shake_128(key.encode('utf-8')).hexdigest(1), 16)\n",
    "            color = matplotlib.colors.hsv_to_rgb((hue / 255, 1, 1))\n",
    "            \n",
    "            # ax.arrow(vector_sum[0] / s, vector_sum[1] / s, activation[0] / s, activation[1] / s, color=\"red\", linewidth=2, head_width=0.02, head_length=0.02)\n",
    "            new_vector_sum = vector_sum + np.array(activation[:2])\n",
    "            ax.plot(\n",
    "                [vector_sum[0] / s, new_vector_sum[0] / s],\n",
    "                [vector_sum[1] / s, new_vector_sum[1] / s],\n",
    "                color=color,\n",
    "                linewidth=2\n",
    "            )\n",
    "            vector_sum = new_vector_sum\n",
    "\n",
    "\n",
    "        # update plot\n",
    "        self.plot.param.trigger('object')\n",
    "\n",
    "ui = UI()\n",
    "ui.full.show(port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consctruct a random set of directions\n",
    "np.random.seed(0)\n",
    "directions = dict()\n",
    "for letter in \"abcdefghijklmnopqrstuvwxyz,./\":\n",
    "    directions[letter] = np.random.normal(0, 1, model.cfg.d_model)\n",
    "# TODO handling these directions could be done by a class, together with looping over pressed keys, and later extracting directions from text\n",
    "    \n",
    "\n",
    "def add_vector(resid_pre, hook):\n",
    "    if hook.layer() != ui.layer_num.value:\n",
    "        return\n",
    "\n",
    "    # print(hook.__dict__)\n",
    "    to_add = np.zeros(model.cfg.d_model)\n",
    "    modifying_activations = []\n",
    "    for key in sorted(key_handler.pressed):\n",
    "        if key in directions:\n",
    "            component = directions[key] * ui.steering_strength.value\n",
    "            to_add += component\n",
    "            modifying_activations.append((component[:2], key))\n",
    "    \n",
    "    ui.update_plot(resid_pre[:, -1, :2].flatten(), modifying_activations)\n",
    "\n",
    "    resid_pre[:, -1, :] += to_add\n",
    "    # TODO double check that this broadcasting works as intended\n",
    "    \n",
    "\n",
    "def new_token_callback(tokens, hooked_transformer):\n",
    "    tokens_to_display = tokens[0][1:]   # remove the BOS token\n",
    "    text = hooked_transformer.tokenizer.decode(tokens_to_display)\n",
    "    ui.text_area.value_input = text\n",
    "\n",
    "\n",
    "def generate_tokens(text):\n",
    "    _hook_filter = lambda name: name.endswith(\"resid_pre\")\n",
    "    with model.hooks(fwd_hooks=[(_hook_filter, add_vector)]):\n",
    "        new_text = model.generate(text, max_new_tokens=999999, temperature=1, verbose=False, stop_criterion=key_handler.should_we_stop_generating, new_token_callback=new_token_callback)\n",
    "        # new_text = model.generate(text, max_new_tokens=1, temperature=1, verbose=False, persist_past_kv_cache=True)\n",
    "    return new_text\n",
    "\n",
    "    \n",
    "def main_loop_func():\n",
    "    key_handler.esc_registered = False\n",
    "    while not key_handler.esc_registered:\n",
    "        ui.info_box.value = str(key_handler.pressed)\n",
    "        if not key_handler.should_we_stop_generating():\n",
    "            generate_tokens(ui.text_area.value_input)\n",
    "            # ui.text_area.value_input = generate_tokens(ui.text_area.value_input)\n",
    "        else:\n",
    "            time.sleep(0.010)\n",
    "    ui.info_box.value = \"Off\"\n",
    "\n",
    "# it needs to be a thread because otherwise panel can't update\n",
    "main_loop = Thread(target=main_loop_func)\n",
    "main_loop.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(['/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- [x] when I press many keys at once, during alt+shift, sometimes keys don't get unpressed\n",
    "- [x] display 2D modificators preview\n",
    "- [x] speedup generation by calling generate once per alt+shift press\n",
    "- [x] fix that nasty panel text update lag\n",
    "    - fixed by generating the plot as svg; now no need to reimplement in dash\n",
    "- [x] toggle generation mode, not only hold to generate mode\n",
    "- [ ] get directions from text, implement a class for that\n",
    "\n",
    "- [ ] fork transformerlens with my mod (to be able to access it in AWS) and later PR\n",
    "\n",
    "- [ ] reusing hooked_transformers cache would be better - start of generation would be faster; also there seems to be some small sync issue with using those callbacks\n",
    "    - the problem is: AssertionError: Pass in one token at a time after loading cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
